{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Linear-to-JavaScript.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pLTlDFwU1Ux_",
        "outputId": "004d7e83-fc55-418a-c91c-e679a265691c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflowjs"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflowjs\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/73/f7ee6edced75b7dfe43916203f1b2e85dd14cba087a090e6372cbd82e462/tensorflowjs-1.4.0-py3-none-any.whl (56kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 30kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.5MB/s \n",
            "\u001b[?25hCollecting PyInquirer==1.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/4c/434b7c454010a284b49d6f1d446fe8dc5960415613d8c0225b9e2efb6724/PyInquirer-1.0.3.tar.gz\n",
            "Collecting tensorflow-hub==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/be/f18c352d84382d9c795a0f37eaf16d42ace7d161fbb0ad20bdcd5e550015/tensorflow_hub-0.5.0-py2.py3-none-any.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.1MB/s \n",
            "\u001b[?25hCollecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 183kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.8.0)\n",
            "Collecting six==1.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow==1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (0.2.2)\n",
            "Collecting prompt_toolkit==1.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/3d/b25d35a9f0d381dd1c02d8e04b37c353caaaff4bc32150328eeebe4931f5/prompt_toolkit-1.0.14-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 46.4MB/s \n",
            "\u001b[?25hCollecting Pygments>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/39/32da3184734730c0e4d3fa3b2b5872104668ad6dc1b5a73d8e477e5fe967/Pygments-2.5.2-py2.py3-none-any.whl (896kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2016.11.21 in /usr/local/lib/python3.6/dist-packages (from PyInquirer==1.0.3->tensorflowjs) (2019.12.9)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub==0.5.0->tensorflowjs) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (0.33.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (0.1.8)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt_toolkit==1.0.14->PyInquirer==1.0.3->tensorflowjs) (0.1.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub==0.5.0->tensorflowjs) (42.0.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->tensorflowjs) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->tensorflowjs) (3.1.1)\n",
            "Building wheels for collected packages: PyInquirer\n",
            "  Building wheel for PyInquirer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyInquirer: filename=PyInquirer-1.0.3-cp36-none-any.whl size=32853 sha256=7e19b6ca84c88346be6e52cb3a1b484d4cb18ab7b0f72ab9804999cb5fa2a0a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/6c/b1/3e4b0e8daf42a92883c7641c0ea8ffb62e0490ebed2faa55ad\n",
            "Successfully built PyInquirer\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, prompt-toolkit, Pygments, PyInquirer, numpy, tensorflow-hub, tensorflowjs\n",
            "  Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: Pygments 2.1.3\n",
            "    Uninstalling Pygments-2.1.3:\n",
            "      Successfully uninstalled Pygments-2.1.3\n",
            "  Found existing installation: numpy 1.17.4\n",
            "    Uninstalling numpy-1.17.4:\n",
            "      Successfully uninstalled numpy-1.17.4\n",
            "  Found existing installation: tensorflow-hub 0.7.0\n",
            "    Uninstalling tensorflow-hub-0.7.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.7.0\n",
            "Successfully installed PyInquirer-1.0.3 Pygments-2.5.2 numpy-1.16.4 prompt-toolkit-1.0.14 six-1.11.0 tensorflow-hub-0.5.0 tensorflowjs-1.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "prompt_toolkit",
                  "pygments",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vNHv1d2K0Wxg",
        "outputId": "1e6cca75-e3da-4ba4-9fd8-9f6d3bdccbb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print('\\u2022 Using TensorFlow Version:', tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "• Using TensorFlow Version: 1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXxi1Zf1dlZg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "251bc2c8-8f5d-4e4c-b6e5-1cdf7a91bec7"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(units=1, input_shape=[1])  \n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "model.fit(xs, ys, epochs=500)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 6 samples\n",
            "Epoch 1/500\n",
            "6/6 [==============================] - 9s 2s/sample - loss: 11.4586\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 455us/sample - loss: 9.2349\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 583us/sample - loss: 7.4809\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 467us/sample - loss: 6.0965\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 515us/sample - loss: 5.0030\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 449us/sample - loss: 4.1384\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 424us/sample - loss: 3.4541\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 439us/sample - loss: 2.9116\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 2.4808\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 434us/sample - loss: 2.1380\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 783us/sample - loss: 1.8644\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 273us/sample - loss: 1.6455\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 465us/sample - loss: 1.4695\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 819us/sample - loss: 1.3275\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 588us/sample - loss: 1.2123\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 705us/sample - loss: 1.1182\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 525us/sample - loss: 1.0407\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 371us/sample - loss: 0.9765\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 464us/sample - loss: 0.9227\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 458us/sample - loss: 0.8773\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 476us/sample - loss: 0.8384\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 455us/sample - loss: 0.8047\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 944us/sample - loss: 0.7753\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 475us/sample - loss: 0.7492\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 311us/sample - loss: 0.7258\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 762us/sample - loss: 0.7046\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 317us/sample - loss: 0.6852\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 355us/sample - loss: 0.6672\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 770us/sample - loss: 0.6505\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 416us/sample - loss: 0.6347\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 713us/sample - loss: 0.6198\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 765us/sample - loss: 0.6056\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 425us/sample - loss: 0.5919\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 328us/sample - loss: 0.5789\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.5662\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 476us/sample - loss: 0.5540\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 492us/sample - loss: 0.5422\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 835us/sample - loss: 0.5307\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 579us/sample - loss: 0.5195\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 428us/sample - loss: 0.5086\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 323us/sample - loss: 0.4980\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 351us/sample - loss: 0.4877\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 497us/sample - loss: 0.4775\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 510us/sample - loss: 0.4676\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 443us/sample - loss: 0.4580\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 501us/sample - loss: 0.4485\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 592us/sample - loss: 0.4393\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 390us/sample - loss: 0.4302\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 666us/sample - loss: 0.4213\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 450us/sample - loss: 0.4127\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 623us/sample - loss: 0.4042\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 444us/sample - loss: 0.3959\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 662us/sample - loss: 0.3877\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 428us/sample - loss: 0.3798\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 441us/sample - loss: 0.3719\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 598us/sample - loss: 0.3643\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 506us/sample - loss: 0.3568\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 464us/sample - loss: 0.3495\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.3423\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 969us/sample - loss: 0.3353\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.3284\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 784us/sample - loss: 0.3216\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 788us/sample - loss: 0.3150\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 700us/sample - loss: 0.3086\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 779us/sample - loss: 0.3022\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 865us/sample - loss: 0.2960\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 827us/sample - loss: 0.2899\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 606us/sample - loss: 0.2840\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 783us/sample - loss: 0.2781\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 821us/sample - loss: 0.2724\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 574us/sample - loss: 0.2668\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 651us/sample - loss: 0.2613\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 621us/sample - loss: 0.2560\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 790us/sample - loss: 0.2507\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 855us/sample - loss: 0.2456\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 694us/sample - loss: 0.2405\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 625us/sample - loss: 0.2356\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 764us/sample - loss: 0.2307\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 697us/sample - loss: 0.2260\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 791us/sample - loss: 0.2214\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 860us/sample - loss: 0.2168\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 504us/sample - loss: 0.2124\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 734us/sample - loss: 0.2080\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 633us/sample - loss: 0.2037\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 585us/sample - loss: 0.1995\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 656us/sample - loss: 0.1954\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 607us/sample - loss: 0.1914\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 628us/sample - loss: 0.1875\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 718us/sample - loss: 0.1836\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 669us/sample - loss: 0.1799\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 654us/sample - loss: 0.1762\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 484us/sample - loss: 0.1726\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 654us/sample - loss: 0.1690\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 646us/sample - loss: 0.1655\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 685us/sample - loss: 0.1621\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 643us/sample - loss: 0.1588\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 593us/sample - loss: 0.1556\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 596us/sample - loss: 0.1524\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 596us/sample - loss: 0.1492\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 633us/sample - loss: 0.1462\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 600us/sample - loss: 0.1432\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 752us/sample - loss: 0.1402\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 630us/sample - loss: 0.1373\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 636us/sample - loss: 0.1345\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 618us/sample - loss: 0.1318\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 522us/sample - loss: 0.1291\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 849us/sample - loss: 0.1264\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 721us/sample - loss: 0.1238\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 782us/sample - loss: 0.1213\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 709us/sample - loss: 0.1188\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 688us/sample - loss: 0.1163\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 617us/sample - loss: 0.1139\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 604us/sample - loss: 0.1116\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 660us/sample - loss: 0.1093\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 732us/sample - loss: 0.1071\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 637us/sample - loss: 0.1049\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 762us/sample - loss: 0.1027\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 625us/sample - loss: 0.1006\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 793us/sample - loss: 0.0985\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 765us/sample - loss: 0.0965\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 729us/sample - loss: 0.0945\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 735us/sample - loss: 0.0926\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 670us/sample - loss: 0.0907\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 716us/sample - loss: 0.0888\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 755us/sample - loss: 0.0870\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 853us/sample - loss: 0.0852\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 637us/sample - loss: 0.0835\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 656us/sample - loss: 0.0817\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 802us/sample - loss: 0.0801\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 596us/sample - loss: 0.0784\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 630us/sample - loss: 0.0768\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 740us/sample - loss: 0.0752\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 613us/sample - loss: 0.0737\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 822us/sample - loss: 0.0722\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 742us/sample - loss: 0.0707\n",
            "Epoch 136/500\n",
            "6/6 [==============================] - 0s 735us/sample - loss: 0.0692\n",
            "Epoch 137/500\n",
            "6/6 [==============================] - 0s 807us/sample - loss: 0.0678\n",
            "Epoch 138/500\n",
            "6/6 [==============================] - 0s 781us/sample - loss: 0.0664\n",
            "Epoch 139/500\n",
            "6/6 [==============================] - 0s 776us/sample - loss: 0.0651\n",
            "Epoch 140/500\n",
            "6/6 [==============================] - 0s 712us/sample - loss: 0.0637\n",
            "Epoch 141/500\n",
            "6/6 [==============================] - 0s 675us/sample - loss: 0.0624\n",
            "Epoch 142/500\n",
            "6/6 [==============================] - 0s 636us/sample - loss: 0.0611\n",
            "Epoch 143/500\n",
            "6/6 [==============================] - 0s 544us/sample - loss: 0.0599\n",
            "Epoch 144/500\n",
            "6/6 [==============================] - 0s 658us/sample - loss: 0.0586\n",
            "Epoch 145/500\n",
            "6/6 [==============================] - 0s 704us/sample - loss: 0.0574\n",
            "Epoch 146/500\n",
            "6/6 [==============================] - 0s 661us/sample - loss: 0.0563\n",
            "Epoch 147/500\n",
            "6/6 [==============================] - 0s 645us/sample - loss: 0.0551\n",
            "Epoch 148/500\n",
            "6/6 [==============================] - 0s 715us/sample - loss: 0.0540\n",
            "Epoch 149/500\n",
            "6/6 [==============================] - 0s 694us/sample - loss: 0.0529\n",
            "Epoch 150/500\n",
            "6/6 [==============================] - 0s 776us/sample - loss: 0.0518\n",
            "Epoch 151/500\n",
            "6/6 [==============================] - 0s 722us/sample - loss: 0.0507\n",
            "Epoch 152/500\n",
            "6/6 [==============================] - 0s 854us/sample - loss: 0.0497\n",
            "Epoch 153/500\n",
            "6/6 [==============================] - 0s 610us/sample - loss: 0.0487\n",
            "Epoch 154/500\n",
            "6/6 [==============================] - 0s 556us/sample - loss: 0.0477\n",
            "Epoch 155/500\n",
            "6/6 [==============================] - 0s 427us/sample - loss: 0.0467\n",
            "Epoch 156/500\n",
            "6/6 [==============================] - 0s 412us/sample - loss: 0.0457\n",
            "Epoch 157/500\n",
            "6/6 [==============================] - 0s 501us/sample - loss: 0.0448\n",
            "Epoch 158/500\n",
            "6/6 [==============================] - 0s 586us/sample - loss: 0.0439\n",
            "Epoch 159/500\n",
            "6/6 [==============================] - 0s 829us/sample - loss: 0.0430\n",
            "Epoch 160/500\n",
            "6/6 [==============================] - 0s 775us/sample - loss: 0.0421\n",
            "Epoch 161/500\n",
            "6/6 [==============================] - 0s 709us/sample - loss: 0.0412\n",
            "Epoch 162/500\n",
            "6/6 [==============================] - 0s 789us/sample - loss: 0.0404\n",
            "Epoch 163/500\n",
            "6/6 [==============================] - 0s 685us/sample - loss: 0.0395\n",
            "Epoch 164/500\n",
            "6/6 [==============================] - 0s 799us/sample - loss: 0.0387\n",
            "Epoch 165/500\n",
            "6/6 [==============================] - 0s 662us/sample - loss: 0.0379\n",
            "Epoch 166/500\n",
            "6/6 [==============================] - 0s 659us/sample - loss: 0.0371\n",
            "Epoch 167/500\n",
            "6/6 [==============================] - 0s 871us/sample - loss: 0.0364\n",
            "Epoch 168/500\n",
            "6/6 [==============================] - 0s 754us/sample - loss: 0.0356\n",
            "Epoch 169/500\n",
            "6/6 [==============================] - 0s 557us/sample - loss: 0.0349\n",
            "Epoch 170/500\n",
            "6/6 [==============================] - 0s 648us/sample - loss: 0.0342\n",
            "Epoch 171/500\n",
            "6/6 [==============================] - 0s 575us/sample - loss: 0.0335\n",
            "Epoch 172/500\n",
            "6/6 [==============================] - 0s 593us/sample - loss: 0.0328\n",
            "Epoch 173/500\n",
            "6/6 [==============================] - 0s 884us/sample - loss: 0.0321\n",
            "Epoch 174/500\n",
            "6/6 [==============================] - 0s 724us/sample - loss: 0.0315\n",
            "Epoch 175/500\n",
            "6/6 [==============================] - 0s 852us/sample - loss: 0.0308\n",
            "Epoch 176/500\n",
            "6/6 [==============================] - 0s 757us/sample - loss: 0.0302\n",
            "Epoch 177/500\n",
            "6/6 [==============================] - 0s 834us/sample - loss: 0.0296\n",
            "Epoch 178/500\n",
            "6/6 [==============================] - 0s 598us/sample - loss: 0.0290\n",
            "Epoch 179/500\n",
            "6/6 [==============================] - 0s 635us/sample - loss: 0.0284\n",
            "Epoch 180/500\n",
            "6/6 [==============================] - 0s 775us/sample - loss: 0.0278\n",
            "Epoch 181/500\n",
            "6/6 [==============================] - 0s 650us/sample - loss: 0.0272\n",
            "Epoch 182/500\n",
            "6/6 [==============================] - 0s 802us/sample - loss: 0.0267\n",
            "Epoch 183/500\n",
            "6/6 [==============================] - 0s 761us/sample - loss: 0.0261\n",
            "Epoch 184/500\n",
            "6/6 [==============================] - 0s 861us/sample - loss: 0.0256\n",
            "Epoch 185/500\n",
            "6/6 [==============================] - 0s 601us/sample - loss: 0.0250\n",
            "Epoch 186/500\n",
            "6/6 [==============================] - 0s 678us/sample - loss: 0.0245\n",
            "Epoch 187/500\n",
            "6/6 [==============================] - 0s 669us/sample - loss: 0.0240\n",
            "Epoch 188/500\n",
            "6/6 [==============================] - 0s 640us/sample - loss: 0.0235\n",
            "Epoch 189/500\n",
            "6/6 [==============================] - 0s 734us/sample - loss: 0.0230\n",
            "Epoch 190/500\n",
            "6/6 [==============================] - 0s 626us/sample - loss: 0.0226\n",
            "Epoch 191/500\n",
            "6/6 [==============================] - 0s 621us/sample - loss: 0.0221\n",
            "Epoch 192/500\n",
            "6/6 [==============================] - 0s 721us/sample - loss: 0.0217\n",
            "Epoch 193/500\n",
            "6/6 [==============================] - 0s 631us/sample - loss: 0.0212\n",
            "Epoch 194/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0208\n",
            "Epoch 195/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0203\n",
            "Epoch 196/500\n",
            "6/6 [==============================] - 0s 576us/sample - loss: 0.0199\n",
            "Epoch 197/500\n",
            "6/6 [==============================] - 0s 703us/sample - loss: 0.0195\n",
            "Epoch 198/500\n",
            "6/6 [==============================] - 0s 673us/sample - loss: 0.0191\n",
            "Epoch 199/500\n",
            "6/6 [==============================] - 0s 617us/sample - loss: 0.0187\n",
            "Epoch 200/500\n",
            "6/6 [==============================] - 0s 620us/sample - loss: 0.0183\n",
            "Epoch 201/500\n",
            "6/6 [==============================] - 0s 476us/sample - loss: 0.0180\n",
            "Epoch 202/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0176\n",
            "Epoch 203/500\n",
            "6/6 [==============================] - 0s 538us/sample - loss: 0.0172\n",
            "Epoch 204/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0169\n",
            "Epoch 205/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0165\n",
            "Epoch 206/500\n",
            "6/6 [==============================] - 0s 466us/sample - loss: 0.0162\n",
            "Epoch 207/500\n",
            "6/6 [==============================] - 0s 797us/sample - loss: 0.0159\n",
            "Epoch 208/500\n",
            "6/6 [==============================] - 0s 767us/sample - loss: 0.0155\n",
            "Epoch 209/500\n",
            "6/6 [==============================] - 0s 780us/sample - loss: 0.0152\n",
            "Epoch 210/500\n",
            "6/6 [==============================] - 0s 692us/sample - loss: 0.0149\n",
            "Epoch 211/500\n",
            "6/6 [==============================] - 0s 770us/sample - loss: 0.0146\n",
            "Epoch 212/500\n",
            "6/6 [==============================] - 0s 782us/sample - loss: 0.0143\n",
            "Epoch 213/500\n",
            "6/6 [==============================] - 0s 740us/sample - loss: 0.0140\n",
            "Epoch 214/500\n",
            "6/6 [==============================] - 0s 763us/sample - loss: 0.0137\n",
            "Epoch 215/500\n",
            "6/6 [==============================] - 0s 992us/sample - loss: 0.0134\n",
            "Epoch 216/500\n",
            "6/6 [==============================] - 0s 585us/sample - loss: 0.0132\n",
            "Epoch 217/500\n",
            "6/6 [==============================] - 0s 986us/sample - loss: 0.0129\n",
            "Epoch 218/500\n",
            "6/6 [==============================] - 0s 709us/sample - loss: 0.0126\n",
            "Epoch 219/500\n",
            "6/6 [==============================] - 0s 726us/sample - loss: 0.0124\n",
            "Epoch 220/500\n",
            "6/6 [==============================] - 0s 616us/sample - loss: 0.0121\n",
            "Epoch 221/500\n",
            "6/6 [==============================] - 0s 695us/sample - loss: 0.0119\n",
            "Epoch 222/500\n",
            "6/6 [==============================] - 0s 715us/sample - loss: 0.0116\n",
            "Epoch 223/500\n",
            "6/6 [==============================] - 0s 670us/sample - loss: 0.0114\n",
            "Epoch 224/500\n",
            "6/6 [==============================] - 0s 525us/sample - loss: 0.0111\n",
            "Epoch 225/500\n",
            "6/6 [==============================] - 0s 461us/sample - loss: 0.0109\n",
            "Epoch 226/500\n",
            "6/6 [==============================] - 0s 448us/sample - loss: 0.0107\n",
            "Epoch 227/500\n",
            "6/6 [==============================] - 0s 441us/sample - loss: 0.0105\n",
            "Epoch 228/500\n",
            "6/6 [==============================] - 0s 629us/sample - loss: 0.0103\n",
            "Epoch 229/500\n",
            "6/6 [==============================] - 0s 635us/sample - loss: 0.0100\n",
            "Epoch 230/500\n",
            "6/6 [==============================] - 0s 742us/sample - loss: 0.0098\n",
            "Epoch 231/500\n",
            "6/6 [==============================] - 0s 778us/sample - loss: 0.0096\n",
            "Epoch 232/500\n",
            "6/6 [==============================] - 0s 723us/sample - loss: 0.0094\n",
            "Epoch 233/500\n",
            "6/6 [==============================] - 0s 640us/sample - loss: 0.0092\n",
            "Epoch 234/500\n",
            "6/6 [==============================] - 0s 623us/sample - loss: 0.0091\n",
            "Epoch 235/500\n",
            "6/6 [==============================] - 0s 2ms/sample - loss: 0.0089\n",
            "Epoch 236/500\n",
            "6/6 [==============================] - 0s 841us/sample - loss: 0.0087\n",
            "Epoch 237/500\n",
            "6/6 [==============================] - 0s 797us/sample - loss: 0.0085\n",
            "Epoch 238/500\n",
            "6/6 [==============================] - 0s 798us/sample - loss: 0.0083\n",
            "Epoch 239/500\n",
            "6/6 [==============================] - 0s 806us/sample - loss: 0.0082\n",
            "Epoch 240/500\n",
            "6/6 [==============================] - 0s 802us/sample - loss: 0.0080\n",
            "Epoch 241/500\n",
            "6/6 [==============================] - 0s 634us/sample - loss: 0.0078\n",
            "Epoch 242/500\n",
            "6/6 [==============================] - 0s 976us/sample - loss: 0.0077\n",
            "Epoch 243/500\n",
            "6/6 [==============================] - 0s 744us/sample - loss: 0.0075\n",
            "Epoch 244/500\n",
            "6/6 [==============================] - 0s 647us/sample - loss: 0.0074\n",
            "Epoch 245/500\n",
            "6/6 [==============================] - 0s 746us/sample - loss: 0.0072\n",
            "Epoch 246/500\n",
            "6/6 [==============================] - 0s 899us/sample - loss: 0.0071\n",
            "Epoch 247/500\n",
            "6/6 [==============================] - 0s 824us/sample - loss: 0.0069\n",
            "Epoch 248/500\n",
            "6/6 [==============================] - 0s 836us/sample - loss: 0.0068\n",
            "Epoch 249/500\n",
            "6/6 [==============================] - 0s 660us/sample - loss: 0.0066\n",
            "Epoch 250/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0065\n",
            "Epoch 251/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0064\n",
            "Epoch 252/500\n",
            "6/6 [==============================] - 0s 744us/sample - loss: 0.0062\n",
            "Epoch 253/500\n",
            "6/6 [==============================] - 0s 542us/sample - loss: 0.0061\n",
            "Epoch 254/500\n",
            "6/6 [==============================] - 0s 619us/sample - loss: 0.0060\n",
            "Epoch 255/500\n",
            "6/6 [==============================] - 0s 793us/sample - loss: 0.0059\n",
            "Epoch 256/500\n",
            "6/6 [==============================] - 0s 711us/sample - loss: 0.0057\n",
            "Epoch 257/500\n",
            "6/6 [==============================] - 0s 749us/sample - loss: 0.0056\n",
            "Epoch 258/500\n",
            "6/6 [==============================] - 0s 863us/sample - loss: 0.0055\n",
            "Epoch 259/500\n",
            "6/6 [==============================] - 0s 722us/sample - loss: 0.0054\n",
            "Epoch 260/500\n",
            "6/6 [==============================] - 0s 716us/sample - loss: 0.0053\n",
            "Epoch 261/500\n",
            "6/6 [==============================] - 0s 340us/sample - loss: 0.0052\n",
            "Epoch 262/500\n",
            "6/6 [==============================] - 0s 717us/sample - loss: 0.0051\n",
            "Epoch 263/500\n",
            "6/6 [==============================] - 0s 582us/sample - loss: 0.0050\n",
            "Epoch 264/500\n",
            "6/6 [==============================] - 0s 910us/sample - loss: 0.0049\n",
            "Epoch 265/500\n",
            "6/6 [==============================] - 0s 530us/sample - loss: 0.0048\n",
            "Epoch 266/500\n",
            "6/6 [==============================] - 0s 479us/sample - loss: 0.0047\n",
            "Epoch 267/500\n",
            "6/6 [==============================] - 0s 957us/sample - loss: 0.0046\n",
            "Epoch 268/500\n",
            "6/6 [==============================] - 0s 433us/sample - loss: 0.0045\n",
            "Epoch 269/500\n",
            "6/6 [==============================] - 0s 571us/sample - loss: 0.0044\n",
            "Epoch 270/500\n",
            "6/6 [==============================] - 0s 507us/sample - loss: 0.0043\n",
            "Epoch 271/500\n",
            "6/6 [==============================] - 0s 876us/sample - loss: 0.0042\n",
            "Epoch 272/500\n",
            "6/6 [==============================] - 0s 875us/sample - loss: 0.0041\n",
            "Epoch 273/500\n",
            "6/6 [==============================] - 0s 944us/sample - loss: 0.0040\n",
            "Epoch 274/500\n",
            "6/6 [==============================] - 0s 927us/sample - loss: 0.0039\n",
            "Epoch 275/500\n",
            "6/6 [==============================] - 0s 762us/sample - loss: 0.0039\n",
            "Epoch 276/500\n",
            "6/6 [==============================] - 0s 828us/sample - loss: 0.0038\n",
            "Epoch 277/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0037\n",
            "Epoch 278/500\n",
            "6/6 [==============================] - 0s 648us/sample - loss: 0.0036\n",
            "Epoch 279/500\n",
            "6/6 [==============================] - 0s 642us/sample - loss: 0.0036\n",
            "Epoch 280/500\n",
            "6/6 [==============================] - 0s 741us/sample - loss: 0.0035\n",
            "Epoch 281/500\n",
            "6/6 [==============================] - 0s 799us/sample - loss: 0.0034\n",
            "Epoch 282/500\n",
            "6/6 [==============================] - 0s 626us/sample - loss: 0.0033\n",
            "Epoch 283/500\n",
            "6/6 [==============================] - 0s 611us/sample - loss: 0.0033\n",
            "Epoch 284/500\n",
            "6/6 [==============================] - 0s 672us/sample - loss: 0.0032\n",
            "Epoch 285/500\n",
            "6/6 [==============================] - 0s 817us/sample - loss: 0.0031\n",
            "Epoch 286/500\n",
            "6/6 [==============================] - 0s 686us/sample - loss: 0.0031\n",
            "Epoch 287/500\n",
            "6/6 [==============================] - 0s 688us/sample - loss: 0.0030\n",
            "Epoch 288/500\n",
            "6/6 [==============================] - 0s 636us/sample - loss: 0.0030\n",
            "Epoch 289/500\n",
            "6/6 [==============================] - 0s 590us/sample - loss: 0.0029\n",
            "Epoch 290/500\n",
            "6/6 [==============================] - 0s 727us/sample - loss: 0.0028\n",
            "Epoch 291/500\n",
            "6/6 [==============================] - 0s 606us/sample - loss: 0.0028\n",
            "Epoch 292/500\n",
            "6/6 [==============================] - 0s 642us/sample - loss: 0.0027\n",
            "Epoch 293/500\n",
            "6/6 [==============================] - 0s 767us/sample - loss: 0.0027\n",
            "Epoch 294/500\n",
            "6/6 [==============================] - 0s 565us/sample - loss: 0.0026\n",
            "Epoch 295/500\n",
            "6/6 [==============================] - 0s 642us/sample - loss: 0.0026\n",
            "Epoch 296/500\n",
            "6/6 [==============================] - 0s 651us/sample - loss: 0.0025\n",
            "Epoch 297/500\n",
            "6/6 [==============================] - 0s 705us/sample - loss: 0.0025\n",
            "Epoch 298/500\n",
            "6/6 [==============================] - 0s 696us/sample - loss: 0.0024\n",
            "Epoch 299/500\n",
            "6/6 [==============================] - 0s 587us/sample - loss: 0.0024\n",
            "Epoch 300/500\n",
            "6/6 [==============================] - 0s 776us/sample - loss: 0.0023\n",
            "Epoch 301/500\n",
            "6/6 [==============================] - 0s 656us/sample - loss: 0.0023\n",
            "Epoch 302/500\n",
            "6/6 [==============================] - 0s 607us/sample - loss: 0.0022\n",
            "Epoch 303/500\n",
            "6/6 [==============================] - 0s 613us/sample - loss: 0.0022\n",
            "Epoch 304/500\n",
            "6/6 [==============================] - 0s 644us/sample - loss: 0.0021\n",
            "Epoch 305/500\n",
            "6/6 [==============================] - 0s 658us/sample - loss: 0.0021\n",
            "Epoch 306/500\n",
            "6/6 [==============================] - 0s 722us/sample - loss: 0.0020\n",
            "Epoch 307/500\n",
            "6/6 [==============================] - 0s 459us/sample - loss: 0.0020\n",
            "Epoch 308/500\n",
            "6/6 [==============================] - 0s 340us/sample - loss: 0.0019\n",
            "Epoch 309/500\n",
            "6/6 [==============================] - 0s 529us/sample - loss: 0.0019\n",
            "Epoch 310/500\n",
            "6/6 [==============================] - 0s 381us/sample - loss: 0.0019\n",
            "Epoch 311/500\n",
            "6/6 [==============================] - 0s 327us/sample - loss: 0.0018\n",
            "Epoch 312/500\n",
            "6/6 [==============================] - 0s 493us/sample - loss: 0.0018\n",
            "Epoch 313/500\n",
            "6/6 [==============================] - 0s 345us/sample - loss: 0.0018\n",
            "Epoch 314/500\n",
            "6/6 [==============================] - 0s 717us/sample - loss: 0.0017\n",
            "Epoch 315/500\n",
            "6/6 [==============================] - 0s 592us/sample - loss: 0.0017\n",
            "Epoch 316/500\n",
            "6/6 [==============================] - 0s 640us/sample - loss: 0.0017\n",
            "Epoch 317/500\n",
            "6/6 [==============================] - 0s 697us/sample - loss: 0.0016\n",
            "Epoch 318/500\n",
            "6/6 [==============================] - 0s 764us/sample - loss: 0.0016\n",
            "Epoch 319/500\n",
            "6/6 [==============================] - 0s 665us/sample - loss: 0.0016\n",
            "Epoch 320/500\n",
            "6/6 [==============================] - 0s 689us/sample - loss: 0.0015\n",
            "Epoch 321/500\n",
            "6/6 [==============================] - 0s 601us/sample - loss: 0.0015\n",
            "Epoch 322/500\n",
            "6/6 [==============================] - 0s 606us/sample - loss: 0.0015\n",
            "Epoch 323/500\n",
            "6/6 [==============================] - 0s 623us/sample - loss: 0.0014\n",
            "Epoch 324/500\n",
            "6/6 [==============================] - 0s 670us/sample - loss: 0.0014\n",
            "Epoch 325/500\n",
            "6/6 [==============================] - 0s 575us/sample - loss: 0.0014\n",
            "Epoch 326/500\n",
            "6/6 [==============================] - 0s 583us/sample - loss: 0.0013\n",
            "Epoch 327/500\n",
            "6/6 [==============================] - 0s 659us/sample - loss: 0.0013\n",
            "Epoch 328/500\n",
            "6/6 [==============================] - 0s 607us/sample - loss: 0.0013\n",
            "Epoch 329/500\n",
            "6/6 [==============================] - 0s 860us/sample - loss: 0.0013\n",
            "Epoch 330/500\n",
            "6/6 [==============================] - 0s 681us/sample - loss: 0.0012\n",
            "Epoch 331/500\n",
            "6/6 [==============================] - 0s 704us/sample - loss: 0.0012\n",
            "Epoch 332/500\n",
            "6/6 [==============================] - 0s 622us/sample - loss: 0.0012\n",
            "Epoch 333/500\n",
            "6/6 [==============================] - 0s 764us/sample - loss: 0.0012\n",
            "Epoch 334/500\n",
            "6/6 [==============================] - 0s 773us/sample - loss: 0.0011\n",
            "Epoch 335/500\n",
            "6/6 [==============================] - 0s 863us/sample - loss: 0.0011\n",
            "Epoch 336/500\n",
            "6/6 [==============================] - 0s 710us/sample - loss: 0.0011\n",
            "Epoch 337/500\n",
            "6/6 [==============================] - 0s 722us/sample - loss: 0.0011\n",
            "Epoch 338/500\n",
            "6/6 [==============================] - 0s 815us/sample - loss: 0.0010\n",
            "Epoch 339/500\n",
            "6/6 [==============================] - 0s 711us/sample - loss: 0.0010\n",
            "Epoch 340/500\n",
            "6/6 [==============================] - 0s 482us/sample - loss: 0.0010\n",
            "Epoch 341/500\n",
            "6/6 [==============================] - 0s 655us/sample - loss: 9.8304e-04\n",
            "Epoch 342/500\n",
            "6/6 [==============================] - 0s 550us/sample - loss: 9.6285e-04\n",
            "Epoch 343/500\n",
            "6/6 [==============================] - 0s 548us/sample - loss: 9.4307e-04\n",
            "Epoch 344/500\n",
            "6/6 [==============================] - 0s 552us/sample - loss: 9.2370e-04\n",
            "Epoch 345/500\n",
            "6/6 [==============================] - 0s 586us/sample - loss: 9.0473e-04\n",
            "Epoch 346/500\n",
            "6/6 [==============================] - 0s 708us/sample - loss: 8.8615e-04\n",
            "Epoch 347/500\n",
            "6/6 [==============================] - 0s 2ms/sample - loss: 8.6794e-04\n",
            "Epoch 348/500\n",
            "6/6 [==============================] - 0s 616us/sample - loss: 8.5011e-04\n",
            "Epoch 349/500\n",
            "6/6 [==============================] - 0s 599us/sample - loss: 8.3265e-04\n",
            "Epoch 350/500\n",
            "6/6 [==============================] - 0s 887us/sample - loss: 8.1555e-04\n",
            "Epoch 351/500\n",
            "6/6 [==============================] - 0s 627us/sample - loss: 7.9880e-04\n",
            "Epoch 352/500\n",
            "6/6 [==============================] - 0s 615us/sample - loss: 7.8239e-04\n",
            "Epoch 353/500\n",
            "6/6 [==============================] - 0s 674us/sample - loss: 7.6631e-04\n",
            "Epoch 354/500\n",
            "6/6 [==============================] - 0s 710us/sample - loss: 7.5058e-04\n",
            "Epoch 355/500\n",
            "6/6 [==============================] - 0s 835us/sample - loss: 7.3516e-04\n",
            "Epoch 356/500\n",
            "6/6 [==============================] - 0s 795us/sample - loss: 7.2006e-04\n",
            "Epoch 357/500\n",
            "6/6 [==============================] - 0s 616us/sample - loss: 7.0527e-04\n",
            "Epoch 358/500\n",
            "6/6 [==============================] - 0s 670us/sample - loss: 6.9078e-04\n",
            "Epoch 359/500\n",
            "6/6 [==============================] - 0s 629us/sample - loss: 6.7659e-04\n",
            "Epoch 360/500\n",
            "6/6 [==============================] - 0s 840us/sample - loss: 6.6269e-04\n",
            "Epoch 361/500\n",
            "6/6 [==============================] - 0s 796us/sample - loss: 6.4908e-04\n",
            "Epoch 362/500\n",
            "6/6 [==============================] - 0s 821us/sample - loss: 6.3575e-04\n",
            "Epoch 363/500\n",
            "6/6 [==============================] - 0s 786us/sample - loss: 6.2269e-04\n",
            "Epoch 364/500\n",
            "6/6 [==============================] - 0s 579us/sample - loss: 6.0990e-04\n",
            "Epoch 365/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 5.9737e-04\n",
            "Epoch 366/500\n",
            "6/6 [==============================] - 0s 567us/sample - loss: 5.8510e-04\n",
            "Epoch 367/500\n",
            "6/6 [==============================] - 0s 636us/sample - loss: 5.7308e-04\n",
            "Epoch 368/500\n",
            "6/6 [==============================] - 0s 712us/sample - loss: 5.6131e-04\n",
            "Epoch 369/500\n",
            "6/6 [==============================] - 0s 582us/sample - loss: 5.4978e-04\n",
            "Epoch 370/500\n",
            "6/6 [==============================] - 0s 620us/sample - loss: 5.3849e-04\n",
            "Epoch 371/500\n",
            "6/6 [==============================] - 0s 725us/sample - loss: 5.2743e-04\n",
            "Epoch 372/500\n",
            "6/6 [==============================] - 0s 622us/sample - loss: 5.1660e-04\n",
            "Epoch 373/500\n",
            "6/6 [==============================] - 0s 684us/sample - loss: 5.0598e-04\n",
            "Epoch 374/500\n",
            "6/6 [==============================] - 0s 669us/sample - loss: 4.9559e-04\n",
            "Epoch 375/500\n",
            "6/6 [==============================] - 0s 599us/sample - loss: 4.8541e-04\n",
            "Epoch 376/500\n",
            "6/6 [==============================] - 0s 753us/sample - loss: 4.7544e-04\n",
            "Epoch 377/500\n",
            "6/6 [==============================] - 0s 860us/sample - loss: 4.6567e-04\n",
            "Epoch 378/500\n",
            "6/6 [==============================] - 0s 755us/sample - loss: 4.5611e-04\n",
            "Epoch 379/500\n",
            "6/6 [==============================] - 0s 757us/sample - loss: 4.4674e-04\n",
            "Epoch 380/500\n",
            "6/6 [==============================] - 0s 605us/sample - loss: 4.3756e-04\n",
            "Epoch 381/500\n",
            "6/6 [==============================] - 0s 665us/sample - loss: 4.2858e-04\n",
            "Epoch 382/500\n",
            "6/6 [==============================] - 0s 540us/sample - loss: 4.1977e-04\n",
            "Epoch 383/500\n",
            "6/6 [==============================] - 0s 663us/sample - loss: 4.1115e-04\n",
            "Epoch 384/500\n",
            "6/6 [==============================] - 0s 651us/sample - loss: 4.0271e-04\n",
            "Epoch 385/500\n",
            "6/6 [==============================] - 0s 708us/sample - loss: 3.9443e-04\n",
            "Epoch 386/500\n",
            "6/6 [==============================] - 0s 705us/sample - loss: 3.8633e-04\n",
            "Epoch 387/500\n",
            "6/6 [==============================] - 0s 655us/sample - loss: 3.7839e-04\n",
            "Epoch 388/500\n",
            "6/6 [==============================] - 0s 786us/sample - loss: 3.7062e-04\n",
            "Epoch 389/500\n",
            "6/6 [==============================] - 0s 629us/sample - loss: 3.6301e-04\n",
            "Epoch 390/500\n",
            "6/6 [==============================] - 0s 613us/sample - loss: 3.5555e-04\n",
            "Epoch 391/500\n",
            "6/6 [==============================] - 0s 643us/sample - loss: 3.4825e-04\n",
            "Epoch 392/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 3.4110e-04\n",
            "Epoch 393/500\n",
            "6/6 [==============================] - 0s 701us/sample - loss: 3.3409e-04\n",
            "Epoch 394/500\n",
            "6/6 [==============================] - 0s 600us/sample - loss: 3.2723e-04\n",
            "Epoch 395/500\n",
            "6/6 [==============================] - 0s 795us/sample - loss: 3.2051e-04\n",
            "Epoch 396/500\n",
            "6/6 [==============================] - 0s 2ms/sample - loss: 3.1392e-04\n",
            "Epoch 397/500\n",
            "6/6 [==============================] - 0s 563us/sample - loss: 3.0748e-04\n",
            "Epoch 398/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 3.0116e-04\n",
            "Epoch 399/500\n",
            "6/6 [==============================] - 0s 689us/sample - loss: 2.9498e-04\n",
            "Epoch 400/500\n",
            "6/6 [==============================] - 0s 486us/sample - loss: 2.8891e-04\n",
            "Epoch 401/500\n",
            "6/6 [==============================] - 0s 414us/sample - loss: 2.8298e-04\n",
            "Epoch 402/500\n",
            "6/6 [==============================] - 0s 444us/sample - loss: 2.7717e-04\n",
            "Epoch 403/500\n",
            "6/6 [==============================] - 0s 447us/sample - loss: 2.7148e-04\n",
            "Epoch 404/500\n",
            "6/6 [==============================] - 0s 813us/sample - loss: 2.6590e-04\n",
            "Epoch 405/500\n",
            "6/6 [==============================] - 0s 466us/sample - loss: 2.6044e-04\n",
            "Epoch 406/500\n",
            "6/6 [==============================] - 0s 835us/sample - loss: 2.5509e-04\n",
            "Epoch 407/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 2.4985e-04\n",
            "Epoch 408/500\n",
            "6/6 [==============================] - 0s 987us/sample - loss: 2.4471e-04\n",
            "Epoch 409/500\n",
            "6/6 [==============================] - 0s 960us/sample - loss: 2.3969e-04\n",
            "Epoch 410/500\n",
            "6/6 [==============================] - 0s 773us/sample - loss: 2.3477e-04\n",
            "Epoch 411/500\n",
            "6/6 [==============================] - 0s 712us/sample - loss: 2.2994e-04\n",
            "Epoch 412/500\n",
            "6/6 [==============================] - 0s 789us/sample - loss: 2.2522e-04\n",
            "Epoch 413/500\n",
            "6/6 [==============================] - 0s 863us/sample - loss: 2.2059e-04\n",
            "Epoch 414/500\n",
            "6/6 [==============================] - 0s 872us/sample - loss: 2.1606e-04\n",
            "Epoch 415/500\n",
            "6/6 [==============================] - 0s 943us/sample - loss: 2.1162e-04\n",
            "Epoch 416/500\n",
            "6/6 [==============================] - 0s 710us/sample - loss: 2.0728e-04\n",
            "Epoch 417/500\n",
            "6/6 [==============================] - 0s 683us/sample - loss: 2.0302e-04\n",
            "Epoch 418/500\n",
            "6/6 [==============================] - 0s 888us/sample - loss: 1.9885e-04\n",
            "Epoch 419/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 1.9477e-04\n",
            "Epoch 420/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 1.9076e-04\n",
            "Epoch 421/500\n",
            "6/6 [==============================] - 0s 865us/sample - loss: 1.8685e-04\n",
            "Epoch 422/500\n",
            "6/6 [==============================] - 0s 738us/sample - loss: 1.8301e-04\n",
            "Epoch 423/500\n",
            "6/6 [==============================] - 0s 719us/sample - loss: 1.7925e-04\n",
            "Epoch 424/500\n",
            "6/6 [==============================] - 0s 811us/sample - loss: 1.7557e-04\n",
            "Epoch 425/500\n",
            "6/6 [==============================] - 0s 956us/sample - loss: 1.7196e-04\n",
            "Epoch 426/500\n",
            "6/6 [==============================] - 0s 929us/sample - loss: 1.6843e-04\n",
            "Epoch 427/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 1.6497e-04\n",
            "Epoch 428/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 1.6158e-04\n",
            "Epoch 429/500\n",
            "6/6 [==============================] - 0s 914us/sample - loss: 1.5826e-04\n",
            "Epoch 430/500\n",
            "6/6 [==============================] - 0s 996us/sample - loss: 1.5501e-04\n",
            "Epoch 431/500\n",
            "6/6 [==============================] - 0s 903us/sample - loss: 1.5183e-04\n",
            "Epoch 432/500\n",
            "6/6 [==============================] - 0s 789us/sample - loss: 1.4871e-04\n",
            "Epoch 433/500\n",
            "6/6 [==============================] - 0s 2ms/sample - loss: 1.4565e-04\n",
            "Epoch 434/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 1.4266e-04\n",
            "Epoch 435/500\n",
            "6/6 [==============================] - 0s 878us/sample - loss: 1.3973e-04\n",
            "Epoch 436/500\n",
            "6/6 [==============================] - 0s 655us/sample - loss: 1.3686e-04\n",
            "Epoch 437/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 1.3405e-04\n",
            "Epoch 438/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 1.3130e-04\n",
            "Epoch 439/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 1.2860e-04\n",
            "Epoch 440/500\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 1.2596e-04\n",
            "Epoch 441/500\n",
            "6/6 [==============================] - 0s 780us/sample - loss: 1.2337e-04\n",
            "Epoch 442/500\n",
            "6/6 [==============================] - 0s 959us/sample - loss: 1.2084e-04\n",
            "Epoch 443/500\n",
            "6/6 [==============================] - 0s 953us/sample - loss: 1.1836e-04\n",
            "Epoch 444/500\n",
            "6/6 [==============================] - 0s 938us/sample - loss: 1.1592e-04\n",
            "Epoch 445/500\n",
            "6/6 [==============================] - 0s 763us/sample - loss: 1.1354e-04\n",
            "Epoch 446/500\n",
            "6/6 [==============================] - 0s 695us/sample - loss: 1.1121e-04\n",
            "Epoch 447/500\n",
            "6/6 [==============================] - 0s 957us/sample - loss: 1.0893e-04\n",
            "Epoch 448/500\n",
            "6/6 [==============================] - 0s 907us/sample - loss: 1.0669e-04\n",
            "Epoch 449/500\n",
            "6/6 [==============================] - 0s 764us/sample - loss: 1.0450e-04\n",
            "Epoch 450/500\n",
            "6/6 [==============================] - 0s 753us/sample - loss: 1.0235e-04\n",
            "Epoch 451/500\n",
            "6/6 [==============================] - 0s 757us/sample - loss: 1.0025e-04\n",
            "Epoch 452/500\n",
            "6/6 [==============================] - 0s 864us/sample - loss: 9.8191e-05\n",
            "Epoch 453/500\n",
            "6/6 [==============================] - 0s 399us/sample - loss: 9.6174e-05\n",
            "Epoch 454/500\n",
            "6/6 [==============================] - 0s 582us/sample - loss: 9.4198e-05\n",
            "Epoch 455/500\n",
            "6/6 [==============================] - 0s 597us/sample - loss: 9.2263e-05\n",
            "Epoch 456/500\n",
            "6/6 [==============================] - 0s 426us/sample - loss: 9.0369e-05\n",
            "Epoch 457/500\n",
            "6/6 [==============================] - 0s 553us/sample - loss: 8.8512e-05\n",
            "Epoch 458/500\n",
            "6/6 [==============================] - 0s 457us/sample - loss: 8.6695e-05\n",
            "Epoch 459/500\n",
            "6/6 [==============================] - 0s 441us/sample - loss: 8.4913e-05\n",
            "Epoch 460/500\n",
            "6/6 [==============================] - 0s 511us/sample - loss: 8.3171e-05\n",
            "Epoch 461/500\n",
            "6/6 [==============================] - 0s 604us/sample - loss: 8.1462e-05\n",
            "Epoch 462/500\n",
            "6/6 [==============================] - 0s 471us/sample - loss: 7.9788e-05\n",
            "Epoch 463/500\n",
            "6/6 [==============================] - 0s 535us/sample - loss: 7.8150e-05\n",
            "Epoch 464/500\n",
            "6/6 [==============================] - 0s 592us/sample - loss: 7.6545e-05\n",
            "Epoch 465/500\n",
            "6/6 [==============================] - 0s 677us/sample - loss: 7.4972e-05\n",
            "Epoch 466/500\n",
            "6/6 [==============================] - 0s 356us/sample - loss: 7.3432e-05\n",
            "Epoch 467/500\n",
            "6/6 [==============================] - 0s 510us/sample - loss: 7.1924e-05\n",
            "Epoch 468/500\n",
            "6/6 [==============================] - 0s 411us/sample - loss: 7.0446e-05\n",
            "Epoch 469/500\n",
            "6/6 [==============================] - 0s 467us/sample - loss: 6.8999e-05\n",
            "Epoch 470/500\n",
            "6/6 [==============================] - 0s 467us/sample - loss: 6.7582e-05\n",
            "Epoch 471/500\n",
            "6/6 [==============================] - 0s 353us/sample - loss: 6.6193e-05\n",
            "Epoch 472/500\n",
            "6/6 [==============================] - 0s 508us/sample - loss: 6.4834e-05\n",
            "Epoch 473/500\n",
            "6/6 [==============================] - 0s 842us/sample - loss: 6.3502e-05\n",
            "Epoch 474/500\n",
            "6/6 [==============================] - 0s 476us/sample - loss: 6.2199e-05\n",
            "Epoch 475/500\n",
            "6/6 [==============================] - 0s 510us/sample - loss: 6.0921e-05\n",
            "Epoch 476/500\n",
            "6/6 [==============================] - 0s 629us/sample - loss: 5.9670e-05\n",
            "Epoch 477/500\n",
            "6/6 [==============================] - 0s 492us/sample - loss: 5.8445e-05\n",
            "Epoch 478/500\n",
            "6/6 [==============================] - 0s 892us/sample - loss: 5.7244e-05\n",
            "Epoch 479/500\n",
            "6/6 [==============================] - 0s 821us/sample - loss: 5.6068e-05\n",
            "Epoch 480/500\n",
            "6/6 [==============================] - 0s 667us/sample - loss: 5.4916e-05\n",
            "Epoch 481/500\n",
            "6/6 [==============================] - 0s 870us/sample - loss: 5.3789e-05\n",
            "Epoch 482/500\n",
            "6/6 [==============================] - 0s 754us/sample - loss: 5.2683e-05\n",
            "Epoch 483/500\n",
            "6/6 [==============================] - 0s 625us/sample - loss: 5.1601e-05\n",
            "Epoch 484/500\n",
            "6/6 [==============================] - 0s 862us/sample - loss: 5.0542e-05\n",
            "Epoch 485/500\n",
            "6/6 [==============================] - 0s 677us/sample - loss: 4.9503e-05\n",
            "Epoch 486/500\n",
            "6/6 [==============================] - 0s 570us/sample - loss: 4.8487e-05\n",
            "Epoch 487/500\n",
            "6/6 [==============================] - 0s 619us/sample - loss: 4.7490e-05\n",
            "Epoch 488/500\n",
            "6/6 [==============================] - 0s 938us/sample - loss: 4.6515e-05\n",
            "Epoch 489/500\n",
            "6/6 [==============================] - 0s 701us/sample - loss: 4.5559e-05\n",
            "Epoch 490/500\n",
            "6/6 [==============================] - 0s 648us/sample - loss: 4.4623e-05\n",
            "Epoch 491/500\n",
            "6/6 [==============================] - 0s 664us/sample - loss: 4.3706e-05\n",
            "Epoch 492/500\n",
            "6/6 [==============================] - 0s 737us/sample - loss: 4.2809e-05\n",
            "Epoch 493/500\n",
            "6/6 [==============================] - 0s 662us/sample - loss: 4.1929e-05\n",
            "Epoch 494/500\n",
            "6/6 [==============================] - 0s 562us/sample - loss: 4.1068e-05\n",
            "Epoch 495/500\n",
            "6/6 [==============================] - 0s 527us/sample - loss: 4.0224e-05\n",
            "Epoch 496/500\n",
            "6/6 [==============================] - 0s 615us/sample - loss: 3.9399e-05\n",
            "Epoch 497/500\n",
            "6/6 [==============================] - 0s 715us/sample - loss: 3.8590e-05\n",
            "Epoch 498/500\n",
            "6/6 [==============================] - 0s 624us/sample - loss: 3.7797e-05\n",
            "Epoch 499/500\n",
            "6/6 [==============================] - 0s 663us/sample - loss: 3.7021e-05\n",
            "Epoch 500/500\n",
            "6/6 [==============================] - 0s 636us/sample - loss: 3.6260e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3a5c7fec18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AxSZ4DzJGEKy",
        "outputId": "647355e0-c167-4a78-ba70-7e523b5bd98d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(model.predict([10.0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[18.982431]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7IWF3HudlZm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "2d139eb1-decb-431a-f930-d61333ee7424"
      },
      "source": [
        "import time\n",
        "saved_model_path = \"./{}.h5\".format(int(time.time()))\n",
        "tf.contrib.saved_model.save_keras_model(model, saved_model_path)\n",
        "#model.save(saved_model_path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: ['train']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "WARNING:tensorflow:Export includes no default signature!\n",
            "INFO:tensorflow:No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: ['eval']\n",
            "WARNING:tensorflow:Export includes no default signature!\n",
            "INFO:tensorflow:No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: ./1577150435.h5/saved_model.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bpBai4DdlZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!tensorflowjs_converter --input_format=keras {saved_model_path} ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbpDtY0-gY_r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ecb4acb4-e431-4ece-9f4f-68b8e2b96f39"
      },
      "source": [
        "!tensorflowjs_converter \\\n",
        "--input_format=keras_saved_model \\\n",
        "1577150435.h5 \\\n",
        "/tmp/linear"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-24 01:28:22.785735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-12-24 01:28:22.789082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-24 01:28:22.789897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-24 01:28:22.790182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-24 01:28:22.792430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-24 01:28:22.794198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-24 01:28:22.794531: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-24 01:28:22.796337: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-24 01:28:22.797343: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-24 01:28:22.801165: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-24 01:28:22.801286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-24 01:28:22.802155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-24 01:28:22.802896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-24 01:28:22.807873: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-12-24 01:28:22.808108: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2de8f40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-24 01:28:22.808142: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-12-24 01:28:22.901988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-24 01:28:22.903426: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2de9100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-24 01:28:22.903458: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2019-12-24 01:28:22.903670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-24 01:28:22.904625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-24 01:28:22.904711: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-24 01:28:22.904746: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-24 01:28:22.904773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-24 01:28:22.904799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-24 01:28:22.904827: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-24 01:28:22.904852: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-24 01:28:22.904895: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-24 01:28:22.904993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-24 01:28:22.905923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-24 01:28:22.906781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-24 01:28:22.906856: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-24 01:28:22.908242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-24 01:28:22.908278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-24 01:28:22.908295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-24 01:28:22.908446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-24 01:28:22.909400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-24 01:28:22.910381: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-12-24 01:28:22.910437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14856 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py:171: load_from_saved_model (from tensorflow.python.keras.saving.saved_model_experimental) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The experimental save and load functions have been  deprecated. Please switch to `tf.keras.models.load_model`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}